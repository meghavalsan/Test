"""Partial dependence plots for regression and classification models. """

# Authors: Peter Prettenhofer
#          Trevor Stephens
# License: BSD 3 clause

from itertools import count
import numbers

import numpy as np
from scipy.stats.mstats import mquantiles

from sklearn.utils.extmath import cartesian
from sklearn.externals.joblib import Parallel, delayed
from sklearn.externals import six
from sklearn.externals.six.moves import map, range, zip
from sklearn.utils import check_array
from sklearn.tree._tree import DTYPE

from sklearn.ensemble._gradient_boosting import _partial_dependence_tree
from sklearn.ensemble.gradient_boosting import BaseGradientBoosting
from sklearn.ensemble.forest import ForestRegressor


__all__ = ['partial_dependence', 'plot_partial_dependence']


def _grid_from_X(X, percentiles=(0.05, 0.95), grid_resolution=100):
    """Generate a grid of points based on the ``percentiles of ``X``.
    The grid is generated by placing ``grid_resolution`` equally
    spaced points between the ``percentiles`` of each column
    of ``X``.
    Parameters
    ----------
    X : ndarray
        The data
    percentiles : tuple of floats
        The percentiles which are used to construct the extreme
        values of the grid axes.
    grid_resolution : int
        The number of equally spaced points that are placed
        on the grid.
    Returns
    -------
    grid : ndarray
        All data points on the grid; ``grid.shape[1] == X.shape[1]``
        and ``grid.shape[0] == grid_resolution * X.shape[1]``.
    axes : seq of ndarray
        The axes with which the grid has been created.
    """
    if len(percentiles) != 2:
        raise ValueError('percentile must be tuple of len 2')
    if not all(0. <= x <= 1. for x in percentiles):
        raise ValueError('percentile values must be in [0, 1]')

    axes = []
    for col in range(X.shape[1]):
        uniques = np.unique(X[:, col])
        if uniques.shape[0] < grid_resolution:
            # feature has low resolution use unique vals
            axis = uniques
        else:
            emp_percentiles = mquantiles(X, prob=percentiles, axis=0)
            # create axis based on percentiles and grid resolution
            axis = np.linspace(emp_percentiles[0, col],
                               emp_percentiles[1, col],
                               num=grid_resolution, endpoint=True)
        axes.append(axis)

    return cartesian(axes), axes


def _exact_partial_dependence(est, target_variables, grid, X, output=None):
    """Calculate the partial dependence of ``target_variables``.
    The function will be calculated by calling the ``predict_proba`` method of
    ``est`` for classification or ``predict`` for regression on ``X`` for every
    point in the grid.
    Parameters
    ----------
    est : BaseEstimator
        A fitted classification or regression model.
    target_variables : array-like, dtype=int
        The target features for which the partial dependency should be
        computed (size should be smaller than 3 for visual renderings).
    grid : array-like, shape=(n_points, len(target_variables))
        The grid of ``target_variables`` values for which the
        partial dependency should be evaluated (either ``grid`` or ``X``
        must be specified).
    X : array-like, shape=(n_samples, n_features)
        The data on which ``est`` was trained.
    output : int, optional (default=None)
        The output index to use for multi-output estimators.
    Returns
    -------
    pdp : array, shape=(n_classes, n_points)
        The partial dependence function evaluated on the ``grid``.
        For regression and binary classification ``n_classes==1``.
    """
    n_samples = X.shape[0]
    pdp = []
    for row in range(grid.shape[0]):
        X_eval = X.copy()
        for i, variable in enumerate(target_variables):
            X_eval[:, variable] = np.repeat(grid[row, i], n_samples)
        try:
            pdp_row = est.predict_proba(X_eval)
        except:
            raise ValueError('Call %s.fit before partial_dependence' %
                             est.__class__.__name__)
        if isinstance(pdp_row, list):
            # Multi-output
            if not 0 <= output < len(pdp_row):
                raise ValueError('Valid output must be specified for '
                                 'multi-output models.')
            pdp_row = pdp_row[output]
        pdp_row = np.log(np.clip(pdp_row, 1e-16, 1))
        pdp_row = np.subtract(pdp_row,
                              np.mean(pdp_row, 1)[:, np.newaxis])
        pdp.append(np.mean(pdp_row, 0))
    pdp = np.array(pdp).transpose()
    if pdp.shape[0] == 2:
        # Binary classification
        pdp = pdp[1, :][np.newaxis]
    elif len(pdp.shape) == 1:
        # Regression
        pdp = pdp[np.newaxis]
    return pdp


def _estimated_partial_dependence(est, target_variables, grid, X, output=None):
    """Calculate the partial dependence of ``target_variables``.
    The function will be calculated by calling the ``predict_proba`` method of
    ``est`` for classification or ``predict`` for regression on the mean of
    ``X``.
    Parameters
    ----------
    est : BaseEstimator
        A fitted classification or regression model.
    target_variables : array-like, dtype=int
        The target features for which the partial dependency should be
        computed (size should be smaller than 3 for visual renderings).
    grid : array-like, shape=(n_points, len(target_variables))
        The grid of ``target_variables`` values for which the
        partial dependency should be evaluated (either ``grid`` or ``X``
        must be specified).
    X : array-like, shape=(n_samples, n_features)
        The data on which ``est`` was trained.
    output : int, optional (default=None)
        The output index to use for multi-output estimators.
    Returns
    -------
    pdp : array, shape=(n_classes, n_points)
        The partial dependence function evaluated on the ``grid``.
        For regression and binary classification ``n_classes==1``.
    """
    n_samples = grid.shape[0]
    X_eval = np.tile(X.mean(0), [n_samples, 1])
    for i, variable in enumerate(target_variables):
        X_eval[:, variable] = grid[:, i]
    try:
        pdp = est.predict_proba(X_eval)
    except:
        raise ValueError('Call %s.fit before partial_dependence' %
                         est.__class__.__name__)
    if isinstance(pdp, list):
        # Multi-output
        if not 0 <= output < len(pdp):
            raise ValueError('Valid output must be specified for '
                             'multi-output models.')
        pdp = pdp[output]
    pdp = np.log(np.clip(pdp, 1e-16, 1))
    pdp = np.subtract(pdp, np.median(pdp, 1)[:, np.newaxis])
    pdp = pdp.transpose()
    if pdp.shape[0] == 2:
        # Binary classification
        pdp = pdp[1, :][np.newaxis]
    return pdp


def partial_dependence(est, target_variables, grid=None, X=None, output=None,
                       percentiles=(0.05, 0.95), grid_resolution=100,
                       method=None):
    """Partial dependence of ``target_variables``.
    Partial dependence plots show the dependence between the joint values
    of the ``target_variables`` and the function represented
    by the ``est``.
    Read more in the :ref:`User Guide <partial_dependence>`.
    Parameters
    ----------
    est : BaseEstimator
        A fitted classification or regression model.
    target_variables : array-like, dtype=int
        The target features for which the partial dependency should be
        computed (size should be smaller than 3 for visual renderings).
    grid : array-like, shape=(n_points, len(target_variables))
        The grid of ``target_variables`` values for which the
        partial dependency should be evaluated (either ``grid`` or ``X``
        must be specified).
    X : array-like, shape=(n_samples, n_features)
        The data on which ``est`` was trained. It is used to generate
        a ``grid`` for the ``target_variables``. The ``grid`` comprises
        ``grid_resolution`` equally spaced points between the two
        ``percentiles``.
    output : int, optional (default=None)
        The output index to use for multi-output estimators.
    percentiles : (low, high), default=(0.05, 0.95)
        The lower and upper percentile used create the extreme values
        for the ``grid``. Only if ``X`` is not None.
    grid_resolution : int, default=100
        The number of equally spaced points on the ``grid``.
    method : {'recursion', 'exact', 'estimated', None}, optional (default=None)
        The method to use to calculate the partial dependence function:
        - If 'recursion', the underlying trees of ``est`` will be recursed to
          calculate the function. Only supported for BaseGradientBoosting and
          ForestRegressor.
        - If 'exact', the function will be calculated by calling the
          ``predict_proba`` method of ``est`` for classification or ``predict``
          for regression on ``X``for every point in the grid. To speed up this
          method, you can use a subset of ``X`` or a more coarse grid.
        - If 'estimated', the function will be calculated by calling the
          ``predict_proba`` method of ``est`` for classification or ``predict``
          for regression on the mean of ``X``.
        - If None, then 'recursion' will be used if ``est`` is
          BaseGradientBoosting or ForestRegressor, and 'exact' used for other
          estimators.
    Returns
    -------
    pdp : array, shape=(n_classes, n_points)
        The partial dependence function evaluated on the ``grid``.
        For regression and binary classification ``n_classes==1``.
    axes : seq of ndarray or None
        The axes with which the grid has been created or None if
        the grid has been given.
    Examples
    --------
    >>> samples = [[0, 0, 2], [1, 0, 0]]
    >>> labels = [0, 1]
    >>> from sklearn.ensemble import GradientBoostingClassifier
    >>> gb = GradientBoostingClassifier(random_state=0).fit(samples, labels)
    >>> kwargs = dict(X=samples, percentiles=(0, 1), grid_resolution=2)
    >>> partial_dependence(gb, [0], **kwargs) # doctest: +SKIP
    (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
    """
    if method is None:
        if isinstance(est, (BaseGradientBoosting, ForestRegressor)):
            method = 'recursion'
        else:
            method = 'exact'
    if (not isinstance(est, (BaseGradientBoosting, ForestRegressor)) and
            method == 'recursion'):
        raise ValueError('est has to be an instance of BaseGradientBoosting or'
                         ' ForestRegressor for the "recursion" method. Try '
                         'using method="exact" or "estimated".')
    if method == 'recursion':
        if len(est.estimators_) == 0:
            raise ValueError('Call %s.fit before partial_dependence' %
                             est.__class__.__name__)
        if isinstance(est, BaseGradientBoosting):
            n_features = est.n_features
        else:
            n_features = est.n_features_
    elif X is None:
        raise ValueError('X is required for method="exact" or "estimated".')
    else:
        n_features = X.shape[1]
    if (grid is None and X is None) or (grid is not None and X is not None):
        raise ValueError('Either grid or X must be specified')

    target_variables = np.asarray(target_variables, dtype=np.int32,
                                  order='C').ravel()

    if any([not (0 <= fx < n_features) for fx in target_variables]):
        raise ValueError('target_variables must be in [0, %d]'
                         % (n_features - 1))

    if X is not None:
        X = check_array(X, dtype=DTYPE, order='C')
        grid, axes = _grid_from_X(X[:, target_variables], percentiles,
                                  grid_resolution)
    else:
        assert grid is not None
        # don't return axes if grid is given
        axes = None
        # grid must be 2d
        if grid.ndim == 1:
            grid = grid[:, np.newaxis]
        if grid.ndim != 2:
            raise ValueError('grid must be 2d but is %dd' % grid.ndim)

    grid = np.asarray(grid, dtype=DTYPE, order='C')
    assert grid.shape[1] == target_variables.shape[0]

    if method == 'recursion':
        if isinstance(est, BaseGradientBoosting):
            n_trees_per_stage = est.estimators_.shape[1]
            n_estimators = est.estimators_.shape[0]
            learning_rate = est.learning_rate
        else:
            n_trees_per_stage = 1
            n_estimators = len(est.estimators_)
            learning_rate = 1.
        pdp = np.zeros((n_trees_per_stage, grid.shape[0],), dtype=np.float64,
                       order='C')
        for stage in range(n_estimators):
            for k in range(n_trees_per_stage):
                if isinstance(est, BaseGradientBoosting):
                    tree = est.estimators_[stage, k].tree_
                else:
                    tree = est.estimators_[stage].tree_
                _partial_dependence_tree(tree, grid, target_variables,
                                         learning_rate, pdp[k])
        if isinstance(est, ForestRegressor):
            pdp /= n_estimators
    elif method == 'exact':
        pdp = _exact_partial_dependence(est, target_variables, grid, X, output)
    elif method == 'estimated':
        pdp = _estimated_partial_dependence(est, target_variables, grid, X,
                                            output)
    else:
        raise ValueError('method "%s" is invalid. Use "recursion", "exact", '
                         '"estimated", or None.' % method)

    return pdp, axes